The purpose of this project is to create a suitable Redshift data model on AWS which can be used to analyse user behaviour based on song play logs and additional song data from a music streaming service, and to desing a pipeline in Apache Airflow to coordinate the process.

### Requirements

* Amazon Web Services (AWS)
* Amazon S3
* Amazon Redshift
* Apache Airflow
* Python 3.6 + 
* boto3
* psycopg2


### Description of Data Sets

The data sets are located in S3 in the following buckets.
Log data: s3://udacity-dend/log_data
Song data: s3://udacity-dend/song_data

#### Song Dataset

The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}

#### Log Dataset

The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.

log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json

The log data set has the following columns:
'artist',  'auth',  'firstName',  'gender',  'itemInSession',  'lastName',  'length',  'level',  'location',  'method',  'page',  'registration',  'sessionId',  'song',  'status',  'ts',  'userAgent',  'userId'

### Description of Data Model:
We create various Redshift tables and insert data into them from the log and song files which are stored in Amazon S3. During this process, we copy data from the files in S3 to staging tables on Redshift, and then insert the data into the final tables from these staging tables. The final tables are stored in a star schema with one fact table between four dimension tables. A short description of the tables follows.

#### Staging tables
1. staging_events: the data from all the log files in S3 are inserted as is into this staging table. Columns: artist, auth, firstName, gender, itemInSession, lastName, length, level, location, method,  page, registration, sessionId, song, status, ts, userAgent, userId.
2. staging_songs: the data from all the song files in S3 are inserted as is into this staging table. Columns: num_songs, artist_id, artist_latitude, artist_longitude, artist_location, artist_name, song_id, title, duration, year.

#### Dimension tables
1. songs: contains information about songs. This is fetched from the song data. Columns: song_id, title, artist_id, year, duration.
2. artists: contains information about the songs' artists. This is fetched from the song data.
3. time: contains information about when songs were played. This information is present in the log files. Columns: start_time, hour, day, week, month, year, weekday.
4. users: a table containing user information. The user information is obtained from the log files. Columns: user_id, first_name, last_name, gender, level.

#### Fact table
1. songplays: contains information pertaining to the event that a song is played. Contains user, time, artist and song info. Columns: play_id, start_time, user_id, level, song_id, artist_id, location, session_id, user_agent.

## Airflow
The project template package contains three major components for the project:

The dag template contains the tasks and the dependencies.
The operators folder with operator templates
A helper class for the SQL transformations

In the DAG, default parameters are added according to the foll. guidelines:
* The DAG does not have dependencies on past runs
* On failure, the tasks are retried 3 times
* Retries happen every 5 minutes
* Catchup is turned off
* Do not email on retry


### Building the operators
We build different operators that will create staging and final tables, stage the data, transform and load the data into fact and dimension tables, and run checks on data quality.

The DAG i.e., the dependency graph (from the Airflow UI) is shown in the image below.

![DAG (dependency graph)](./Airflow_DAG.png "DAG (dependency graph)")

All of the operators and task instances will run SQL statements against the Redshift database. However, our goal is to use parameters in a way that allows us build flexible, reusable, and configurable operators that we can later apply to many kinds of data pipelines with Redshift and with other databases.

#### Create tables operator
This operator creates all the tables described in the previous section in Redshift. It takes a .sql file containing all the create table statements as an argument.

#### Stage Operator
The stage operator is designed to be able to load any JSON formatted files from S3 to Amazon Redshift. The operator creates and runs a SQL COPY statement based on the parameters provided. The operator's parameters specify from where in S3 the file is loaded and what the target table is. This operator also allows for uploading CSVs, in case the input data format changes.

The parameters are used distinguish between the different JSON files. Another important requirement satisfied by the stage operator is that it contains a templated field that allows it to load timestamped files from S3 based on the execution time and run backfills.

Note that each time we stage new data, the previous version of the data is deleted.truncate-insert pattern that is normally used with staging tables. This is the  For example, if we run the DAG to stage data from 2 November 2, 2018, this data is removed before staging the data from November 3, 2018.

#### Fact and Dimension Operators
With dimension and fact operators, we utilise the provided SQL helper class to run data transformations. Most of the logic is within the SQL transformations and the operator takes as input a SQL statement and target database on which to run the query against. 

While doing the Dimension loads, there is an option to use the truncate-insert pattern (where the target table is emptied before the load) or the append pattern. There is a parameter that allows switching between insert modes when loading dimensions. No such parameter is provided for loading the fact tables as fact tables are usually so massive that they should only allow append type functionality.

#### Data Quality Operator
The final operator is the data quality operator, which is used to run checks on the data itself. The operator's main functionality is to receive one or more SQL based test cases along with the expected results and execute the tests. For each test, the test result and expected result is checked and if there is no match, the operator raises an exception so the task can retry and fail eventually.

Our data quality checks check for empty tables, but more checks can easily be added.

### Description of files.
The log and song data files are present in S3. They are all Json (Jsonline) files. An overview of the different files in the repository is given below. 

#### Python programs (.py files)
1. dags/sparkify_dag.py: the main file which defines the DAG and the operators, sets the default parameters and task dependencies.
2. plugins/helpers/sql_queries.py: contains SQL INSERT INTO statments. These are used to insert data from the staging tables into the fact and dimension tables.
3. plugins/operators/stage_redshift.py: Airflow custom operator to stage data in Redshift from the source S3 files. This is done via COPY statements.
4. plugins/operators/load_fact.py: Airflow custom operator to load the fact table songplays from the two staging tables.
5. plugins/operators/load_dimension.py: Airflow custom operator to load the four dimension tables from the two staging tables.
6. plugins/operators/data_quality.py: Airflow custom operator to perform data quality checks: check if data has been inserted into the table being loaded.
7. The __init__.py files contain all the required import statements for the operator and helper classes so that they can be directly imported in the main Airflow dag program.

#### SQL files
1. dags/create_tables.sql: contains SQL DDL statements to create all the dimension and fact tables, as well as the staging tables. This is in the same directory as the program which defines the DAG as it needs to be imported into the operator. This is because the create tables operator defined in dags/sparkfiy_dag.py is not a custom operator, but just a simple PostgresOperator.

### Execution
1. On AWS, create an IAM role so our Airflow programs are allowed to read from S3 and write into Redshift.
2. Add an inbound rule so that the cluster we create can be accessed over the TCP protocol. This is done in a security group on AWS.
3. Create a Redshift cluster on AWS. Make sure to use the created IAM rule and security group, and to make it publicly accessible. The main database schema, port, username and password need to be set.
4. Run /opt/airflow/start.sh to start the Airflow service.
5. Open the Airflow UI.
5. Define connections in the airflow UI for Redshift (Conn type=postgres, schema, port, username and password) and AWS (public key and secret key).
6. Turn the DAG 'on'. All the scheduling and handling of retries has been done in the code, so there is nothing further that has to be done on the UI. It can be used to check task progress and to view the logs if any tasks fail.